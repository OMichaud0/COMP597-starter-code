# COMP597 Starter Code
This repository contains starter code for COMP597: Sustainability in Systems Design - Energy Efficiency analysis using CodeCarbon. 

The starter code provides the basics to train a machine learning model using PyTorch. More precisely, the provided code is a command line tool that is designed to be easily extended with new features. It provides the basics to add models, add command line arguments for configuration purposes, add data collection methods, or modify the training loop. 

The repository also provides you with the means to run the code both locally and on Slurm. The expectations are that the Slurm nodes managed by the schools will be used for the final experiments, but if you have a GPU with Cuda and wish to test your code locally, you will find everything you need to do so as well (assuming a Linux host).

## Getting Started

As mentioned above, the provided code is a command line tool. The entry point is the `launch.py` file, located at the root of this repository. Running the `python3 launch.py --help` (locally) or `srun.sh --help` (Slurm) with print a basic help message listing all the possible flags that can be used to configure the execution of a training loop. 

Before digging straight into the code, visit the [documentation](docs/ToC.md). It provides details about the provided code, the required Python environment, how to use Slurm in the context of this project, and how to extend the code provided. 

## Models

| Model Name | Type | Architecture | Size | Documentation | Dataset | Pretrained Weights | Notes |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| BERT | NLP | Transformer | 116M | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/bert) | [Synthetic Dataset from MilaBench](https://github.com/mila-iqia/milabench/blob/master/benchmarks/huggingface/bench/synth.py) | [HuggingFace BERT Model Card](https://huggingface.co/google-bert/bert-base-uncased) | {this pretrained model is 0.1B and it's from huggingface. milabench uses the dataset is synthetic for these [models](https://github.com/mila-iqia/milabench/blob/master/benchmarks/huggingface/prepare.py) but the huggingface model card also has the dataset it was pretrained on.} |
| Reformer | NLP | Transformer | 6M | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/reformer) | [Synthetic Dataset from MilaBench](https://github.com/mila-iqia/milabench/blob/master/benchmarks/huggingface/bench/synth.py) | [HuggingFace Model](https://huggingface.co/docs/transformers/en/model_doc/reformer) | {ReformerConfig(). same as BERT for the dataset?} |
| T5 | NLP | Transformer | 0.2B | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/t5) | [Synthetic Dataset from MilaBench](https://github.com/mila-iqia/milabench/blob/master/benchmarks/huggingface/bench/synth.py) | [HuggingFace T5 Base Model Card](https://huggingface.co/google-t5/t5-base) | {same dataset as BERT?} |
| OPT | NLP | Transformer | 350M | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/opt) | [TODO]() | [HuggingFace Opt-350M Model Card](https://huggingface.co/facebook/opt-350m) | {i am assuming its the same dataset as BERT cz its BERT adjacent models but also im not sure?} |
| Bart | NLP | Transformer | 0.1B | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/bart) | [TODO]() | [HuggingFace Bart Base Model Card](https://huggingface.co/facebook/bart-base) | {i am assuming its the same dataset as BERT cz its BERT adjacent models but also im not sure?} |
| BigBird | NLP | Transformer | ? | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/big_bird) | [TODO]() | [HuggingFace BigBird Roberta Base Model Card](https://huggingface.co/google/bigbird-roberta-base) | {milabench is using BigBirdConfig(attention_type="block_sparse"). i am assuming its the same dataset as BERT cz its BERT adjacent models but also im not sure?} |
| Albert | NLP | Transformer | 11.8M | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/albert) | [TODO]() | [HuggingFace Albert Base V2 Model Card](https://huggingface.co/albert/albert-base-v2) | {i am assuming its the same dataset as BERT cz its BERT adjacent models but also im not sure?} |
| DistilBERT | NLP | Transformer | 67M | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/distilbert) | [TODO]() | [HuggingFace DistilBERT Base Uncased Model Card](https://huggingface.co/docs/transformers/en/model_doc/distilbert) | {i am assuming its the same dataset as BERT cz its BERT adjacent models but also im not sure?} |
| Longformer | NLP | Transformer | ? | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/longformer) | [TODO]() | [HuggingFace Longformer Base 4096 Model Card](https://huggingface.co/allenai/longformer-base-4096) | {i am assuming its the same dataset as BERT cz its BERT adjacent models but also im not sure?} |
| Llava | MultiModal (NLP/CV) | Transformer | ? | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/llava) | [HuggingFace The Cauldron Dataset](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron) | [HuggingFace <MODEL> Model Card]() | {i couldnt find a pretrained model small enough? milabench is using the llava-hf/llava-1.5-7b-hf model} |
| Whisper | ASR | Transformer | 37.8M | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/whisper) | [Synthetic Dataset from MilaBench](https://github.com/mila-iqia/milabench/blob/master/benchmarks/huggingface/bench/synth.py) | [HuggingFace Whisper Tiny Model Card](https://huggingface.co/openai/whisper-tiny) | {same as BERT for dataset?} |
| Dinov2 | ViT | Transformer | 0.3B | [HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/dinov2) | [FakeImageNet](https://huggingface.co/datasets/InfImagine/FakeImageDataset) | [HuggingFace Dinov2 Large Model Card](https://huggingface.co/facebook/dinov2-large) | {the model file uses this [dataset](https://huggingface.co/datasets/helenlu/ade20k) but the table of the models you sent me has the FakeImageNet?} |
| V-Jepa2 | CV | Transformer | 632M | [HuggingFace Documentation](https://huggingface.co/docs/transformers/main/model_doc/vjepa2) | [MilaBench FakeVideo Dataset Generation](https://github.com/mila-iqia/milabench/blob/master/benchmarks/vjepa/prepare.py) | [HuggingFace V-JEPA2 Model Card](https://huggingface.co/facebook/vjepa2-vitl-fpc64-256) | {the dataset is generated by milabench i think} |
| ResNet50 | CV | CNN | 26M | [Pytorch Model Documentation](https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html#resnet50) | [FakeImageNet](https://huggingface.co/datasets/InfImagine/FakeImageDataset) | [TODO]() | {does pytorch models have a model card? i put the model config page in the documentation for now.} |
| Resnet152 | CV | CNN | 60M | [Pytorch Model Documentation](https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.resnet152.html#resnet152) | [FakeImageNet](https://huggingface.co/datasets/InfImagine/FakeImageDataset) | [TODO]() | {does pytorch models have a model card? i put the model config page in the documentation for now.} |
| ConvNext Large | CV | CNN | 200M | [Pytorch Model Documentation](https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.convnext_large.html#convnext-large) | [FakeImageNet](https://huggingface.co/datasets/InfImagine/FakeImageDataset) | [TODO]() | {does pytorch models have a model card? i put the model config page in the documentation for now.} |
| RegNet Y 128GF | CV | CNN,RNN | 693M | [Pytorch Model Documentation](https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.regnet_y_128gf.html#regnet-y-128gf) | [FakeImageNet](https://huggingface.co/datasets/InfImagine/FakeImageDataset) | [TODO]() | {does pytorch models have a model card? i put the model config page in the documentation for now.} |
| ViT-g/14 | CV | Transformer | 1B | [TODO]() | [FakeImageNet](https://huggingface.co/datasets/InfImagine/FakeImageDataset) | [TODO]() | {im a little confused by this one but here is the [huggingface dinov2-giant model](https://huggingface.co/facebook/dinov2-giant). the paper says its dinov2-giant-gpus} |
| PNA | Graphs | GNN | 4M | [TODO]() | [PCQM4Mv2](https://pytorch-geometric.readthedocs.io/en/2.7.0/generated/torch_geometric.datasets.PCQM4Mv2.html) | [TODO]() | {[theres a link to the paper where this model is spawned from](https://github.com/mila-iqia/milabench/blob/master/benchmarks/geo_gnn/bench/models.py): [paper](https://arxiv.org/pdf/2004.05718). the dataset used seems to be a [subset](https://github.com/mila-iqia/milabench/blob/master/benchmarks/geo_gnn/pcqm4m_subset.py)} |
| DimeNet | Graphs | GNN | 500K | [TODO]() | [PCQM4Mv2](https://pytorch-geometric.readthedocs.io/en/2.7.0/generated/torch_geometric.datasets.PCQM4Mv2.html) | [TODO]() | {[theres a link to the paper where this model is spawned from](https://github.com/mila-iqia/milabench/blob/master/benchmarks/geo_gnn/bench/models.py): [paper](https://arxiv.org/pdf/2003.03123). the dataset used seems to be a [subset](https://github.com/mila-iqia/milabench/blob/master/benchmarks/geo_gnn/pcqm4m_subset.py)} |
| GFlowNet | Graphs | GFlowNet, T. | 600M | [TODO]() | [TODO]() | [TODO]() | {[this is the paper about the model](https://arxiv.org/pdf/2106.04399) and [this is the github for the model implementation](https://github.com/GFNOrg/gflownet). they talk about a molecule dataset synthetically generated, are we using that as well? i did not put any model info for this one but they get the model from the github linked here and all other info are [here](https://github.com/mila-iqia/milabench/tree/master/benchmarks/recursiongfn)} |
| ? | ? | ? | ? | [TODO]() | [TODO]() | [TODO]() |

---


## CodeCarbon Resources
- [CodeCarbon Colab Tutorial](https://colab.research.google.com/drive/1eBLk-Fne8YCzuwVLiyLU8w0wNsrfh3xq)
- [CodeCarbon documentation](https://mlco2.github.io/codecarbon/)

---


